# -*- coding: utf-8 -*-
"""SentAnalysis_SML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aK7IIyIyl6v3RLmlewDP87UBZxKmLAuN
"""

from google.colab import files

  # Upload csv file
uploaded = files.upload()

import pandas as pd

  # Read the Dataset and
df = pd.read_csv('social_media_sentiment.csv')

df.head()

df.info()

df.describe()

df.shape

import re
import nltk
from nltk.corpus import stopwords

nltk.download('stopwords')

stop_words = set(stopwords.words('english'))

def clean_text(text):
    text = text.lower()                                     # lowercase
    text = re.sub(r"http\S+", "", text)                     # remove URLs
    text = re.sub(r"@\w+", "", text)                        # remove mentions
    text = re.sub(r"#\w+", "", text)                        # remove hashtags
    text = re.sub(r"[^a-zA-Z\s]", "", text)                 # remove emojis and symbols

    words = text.split()
    words = [w for w in words if w not in stop_words]
    return " ".join(words)

df['clean_text'] = df['text'].apply(clean_text)

!pip install gensim # For

from gensim.models import Word2Vec
import numpy as np

sentences = [text.split() for text in df['clean_text']]

w2v_model = Word2Vec(
    sentences,
    vector_size = 20,
    window = 5,
    min_count = 1,
    workers = 4
)

def vectorize(text):
  words = text.split()
  word_vecs = [w2v_model.wv[word] for word in words if word in w2v_model.wv]
  if len(word_vecs) == 0:
    return np.zeros(w2v_model.vector_size) # Return a zero vector of the correct size if no words are found
  return np.mean(word_vecs, axis=0) # Return the mean of word vectors

X = np.array([vectorize(text) for text in df['clean_text']])
y = df['label']

print(X.shape)
print(y.shape)

from sklearn.model_selection import train_test_split

# Split the Dataset before Training the Models

X_train, X_test, y_train, y_test = train_test_split(
     X, y,
     test_size=0.2,
     random_state=42
 )

# Model 1: Random Forest

from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(
    n_estimators = 100,
    random_state = 42
)

rf.fit(X_train, y_train)

y_pred_rf = rf.predict(X_test)

# Model 2: Improved Random Forest

from sklearn.ensemble import RandomForestClassifier

# Refined Classifiers for Better Performance and Result
refined_rf = RandomForestClassifier(
    n_estimators=300,
    max_depth=20,
    min_samples_split=5,
    random_state=42
)

refined_rf.fit(X_train, y_train)

refined_y_pred_rf = refined_rf.predict(X_test)

# Model 3: Naive Bayes

from sklearn.naive_bayes import GaussianNB

nb = GaussianNB()
nb.fit(X_train, y_train)

y_pred_nb = nb.predict(X_test)

# Print Output of Models for Evaluation

from sklearn.metrics import accuracy_score, classification_report

print("=== Model 1: Random Forest ===")
print("Accuracy:", accuracy_score(y_test, y_pred_rf))
print(classification_report(y_test, y_pred_rf))

print("\n=== Model 2: Improved Random Forest ===")
print("Accuracy:", accuracy_score(y_test, refined_y_pred_rf))
print(classification_report(y_test, refined_y_pred_rf))

print("\n=== Model 3: Naive Bayes ===")
print("Accuracy:", accuracy_score(y_test, y_pred_nb))
print(classification_report(y_test, y_pred_nb))

